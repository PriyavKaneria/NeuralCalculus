---
title: Large Human Language Model
categories: [tech]
tags: [llm, model, life]
---

every person on earth is born with a base model, a set of runtime training parameters and a set context length

Base model depends on their parent's base models and or maybe their past iteration base model remains same (if we believe models reincarnate). Runtime training parameters are our body organs and senses that determine how much our models can interact with the environment. The set context length is the memory capacity which is NOT how much one can memorize but in a way the aptitude capability of the model to relate between different unrelated concepts

Most models tend to fine-tune themselves throughout their lives for some specific task and survive. Some try to be AGI and that is even better. Problems arise when a fundamentally different architecture model dissapoints itself on a benchmark not meant for it. Think what would gpt2 feel running against o1 on the agi benchmark.

A person's motto in life should be to optimize on thier goals according to their model, paramters and context length.

> A point I want to mention here is that a fine-tuned gpt2 for a very specific task can still be better at o1 by expertise given enough data - 10000hrs equivalent